{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Kanellakis Kosmas\n",
    "\n",
    "Project: Clio Muse data manipulation - analysis\n",
    "\n",
    "Professor: Aikatini Pramatari\n",
    "\n",
    "University: Athens University of Economics and Business\n",
    "\n",
    "Department: Management Science and Technology\n",
    "\n",
    "Date: 12/09/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import what you have to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary checks and variables\n",
    "\n",
    "* make folder called outputfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('outputfiles'):\n",
    "    os.makedirs('outputfiles')\n",
    "\n",
    "output_loc = './outputfiles/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the data and make the starting dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files():\n",
    "    #read dataframes from the excel files\n",
    "\n",
    "    try:\n",
    "        if os.path.exists('dataframe1.xlsx'):\n",
    "            dataframe1 = pd.read_excel('dataframe1.xlsx')\n",
    "        else:\n",
    "            dataframe1 = combine_review_sheets()\n",
    "            \n",
    "        if os.path.exists('dataframe2.xlsx'):\n",
    "            dataframe2 = pd.read_excel('dataframe2.xlsx')\n",
    "        else:\n",
    "            dataframe2 = combine_booking_sheets()\n",
    "            #add the ticket cost\n",
    "            dataframe2 = add_ticket_cost(dataframe2)\n",
    "            #get the Revenue\n",
    "            dataframe2 = Revenue(dataframe2)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    return dataframe1, dataframe2\n",
    "\n",
    "def combine_review_sheets():\n",
    "    #read and instantiate dataframe\n",
    "\n",
    "    # File path to your Excel file\n",
    "    file_path = 'reviews data.xlsx'  # Replace with your actual file path\n",
    "\n",
    "    #load Excel file\n",
    "    xlsx = pd.ExcelFile(file_path)\n",
    "\n",
    "    #collect all unique column titles from each sheet\n",
    "    all_columns = []\n",
    "\n",
    "    #iterate through each sheet to collect column names\n",
    "    for sheet_name in xlsx.sheet_names:\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name, header=1, nrows=0)  # Read only the second row for column names\n",
    "        all_columns.extend([col for col in df.columns if col not in all_columns and not 'Unnamed' in str(col)])\n",
    "\n",
    "    #initialize an empty DataFrame to store combined data\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    #iterate through each sheet\n",
    "    for sheet_name in xlsx.sheet_names:\n",
    "        # Read the sheet into a DataFrame, starting from the second row for data\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name, header=1)\n",
    "\n",
    "        # Iterate through each row to find the first entirely empty row\n",
    "        for i in range(len(df)):\n",
    "            if pd.isna(df.iloc[i]).all():  # Check if all elements in the row are NaN\n",
    "                break\n",
    "        # Keep only the data above the first entirely empty row\n",
    "        df = df.iloc[:i]\n",
    "\n",
    "        # Add a column to indicate the source sheet\n",
    "        df['Source Sheet'] = sheet_name\n",
    "        \n",
    "        # Append this DataFrame to the combined DataFrame\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True, sort=False)\n",
    "\n",
    "    # Reindex the combined DataFrame to include all collected columns plus the Source Sheet column\n",
    "    combined_df = combined_df.reindex(columns=all_columns + ['Source Sheet'])\n",
    "\n",
    "    #in the first column of combined_df turn \"0\" to \"FALSE\" and \"1\" to \"TRUE\"\n",
    "    combined_df['Important Information'] = combined_df['Important Information'].replace({0: 'FALSE', 1: 'TRUE'})\n",
    "\n",
    "    #rename the dataframe to dataframe1\n",
    "    dataframe1 = combined_df\n",
    "    \n",
    "    #split recommended['Name of Product Reviewed'] to 2 columns seperated by '|'\n",
    "    dataframe1[['product_code', 'product_review']] = dataframe1['Name of Product Reviewed'].str.split('|', expand=True)\n",
    "    \n",
    "    #drop the column Name of Product Reviewed\n",
    "    dataframe1.drop(columns=['Name of Product Reviewed'], inplace=True)\n",
    "    \n",
    "    #drop the rows where product_code column is na\n",
    "    dataframe1.dropna(subset=['product_code'], inplace=True)\n",
    "    \n",
    "    #drop the rows where product_code do not have in them \"STL,TO,AU,TL\"\n",
    "    dataframe1 = dataframe1[dataframe1['product_code'].str.startswith(('STL', 'TO', 'AU', 'TL'))]\n",
    "    \n",
    "    #for the column \"Overall Experience\" for each row keep only the number\n",
    "    dataframe1.loc[:, 'Overall Experience'] = dataframe1['Overall Experience'].str.extract('(\\d+)', expand=False)\n",
    "    \n",
    "    #remove the spaces from the product_code column\n",
    "    dataframe1['product_code'] = dataframe1['product_code'].str.strip()\n",
    "    \n",
    "    dataframe1.to_excel('dataframe1.xlsx', index = False)\n",
    "    \n",
    "\n",
    "    \n",
    "    return dataframe1\n",
    "\n",
    "def combine_booking_sheets():\n",
    "    # File path to your Excel file\n",
    "    file_path = 'Booking Stats.xlsx'  # Replace with your actual file path\n",
    "\n",
    "    # List of month names to include\n",
    "    month_names = [\n",
    "        'January', 'February', 'March', 'April', 'May', 'June',\n",
    "        'July', 'August', 'September', 'October', 'November', 'December'\n",
    "    ]\n",
    "\n",
    "    # Load Excel file\n",
    "    xlsx = pd.ExcelFile(file_path)\n",
    "\n",
    "    # Initialize an empty DataFrame to store combined data\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate through each sheet\n",
    "    for sheet_name in xlsx.sheet_names:\n",
    "        if sheet_name in month_names:  # Only combine if the sheet is a month\n",
    "            # Read the sheet into a DataFrame, using the first row as the header\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "            \n",
    "            # Add a column to indicate the source sheet\n",
    "            df['Source Sheet'] = sheet_name\n",
    "            \n",
    "            # Append this DataFrame to the combined DataFrame\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    \n",
    "    # Rename the combined DataFrame to dataframe2\n",
    "    dataframe2 = combined_df\n",
    "    \n",
    "    # sort dataframe2 by id\n",
    "    dataframe2 = dataframe2.sort_values(by='id')\n",
    "\n",
    "    dataframe2 = manipulate_dataframe2(dataframe2)\n",
    "        \n",
    "    return dataframe2\n",
    "\n",
    "def manipulate_dataframe2(dataframe2):\n",
    "    # Language codes mapping based on the provided information\n",
    "    language_codes = {\n",
    "        'Greek': 'GR', 'English': 'EN', 'Chinese': 'CH', 'Italian': 'IT',\n",
    "        'German': 'DE', 'French': 'FR', 'Russian': 'RU', 'Spanish': 'ES',\n",
    "        'Romanian': 'RO', 'Serbian': 'SR', 'Turkish': 'TR', 'Hebrew': 'HE',\n",
    "        'Czech': 'CS', 'Hungarian': 'HU', 'Polish': 'PL', 'Bosnian': 'BS',\n",
    "        'Albanian': 'SQ', 'Irish': 'GA', 'Norwegian': 'NO', 'Portuguese': 'PT',\n",
    "        'Korean': 'KO', 'Japanese': 'JA'\n",
    "    }\n",
    "\n",
    "    # Function to map full language name to language code\n",
    "    def map_language_to_code(language):\n",
    "        return language_codes.get(language, None)\n",
    "\n",
    "    # Apply the function to the 'Language' column to create a new 'Language Code' column\n",
    "    # Replace 'LanguageColumnName' with the actual name of the column in dataframe2 that contains language names\n",
    "    dataframe2['Language Code'] = dataframe2['language'].apply(map_language_to_code)\n",
    "\n",
    "    return dataframe2\n",
    "\n",
    "def add_ticket_cost(dataframe2):\n",
    "    \n",
    "    # Load the \"ticket cost\" sheet into a DataFrame\n",
    "    ticket_cost_df = pd.read_excel('Booking Stats.xlsx', sheet_name='Ticket Cost')\n",
    "    \n",
    "    # Set the product code as the index for easier access\n",
    "    ticket_cost_df.set_index('Product Code', inplace=True)\n",
    "        \n",
    "    # Drop the rows where net_price column is na\n",
    "    dataframe2.dropna(subset=['net_price'], inplace=True)\n",
    "    \n",
    "    # Concatenate the product_code and Language Code\n",
    "    dataframe2['Full Product Code'] = dataframe2['product_code'] + dataframe2['Language Code']\n",
    "    \n",
    "    # Iterate over each row in dataframe2\n",
    "    for index, row in dataframe2.iterrows():\n",
    "        # Get the full product code for the current row\n",
    "        full_product_code = row['Full Product Code']\n",
    "        # Get the month for the current row, assuming the 'month' column format is 'Month YYYY'\n",
    "        month = row['month'].split()[0]  # Take only the first part, which is the month name\n",
    "        \n",
    "        # Find the price in the ticket cost dataframe\n",
    "        if full_product_code in ticket_cost_df.index:\n",
    "            # Extract the price for the corresponding month\n",
    "            price = ticket_cost_df.at[full_product_code, month]\n",
    "            # Check if the price is a Series or a single value\n",
    "            if isinstance(price, pd.Series):\n",
    "                price = price.iloc[0]  # Take the first element of the Series\n",
    "            elif isinstance(price, str):\n",
    "                # Clean up the price to be a float (remove '€' and convert to float)\n",
    "                price = float(price.replace('€', ''))\n",
    "        else:\n",
    "            # If the product code is not found, set the price to None or a default value\n",
    "            price = 0\n",
    "        \n",
    "        dataframe2.at[index, 'Ticket Price'] = price\n",
    "    \n",
    "    #drop the 'Full Product Code' column\n",
    "    dataframe2.drop(columns=['Full Product Code'], inplace=True)\n",
    "    \n",
    "    #fill the NaN values with 0\n",
    "    dataframe2['Ticket Price'].fillna(0, inplace=True)\n",
    "        \n",
    "    return dataframe2\n",
    "\n",
    "def Revenue(dataframe2):\n",
    "        \n",
    "    #calculate the profit\n",
    "    dataframe2['Revenue'] = dataframe2['retail_price'] - dataframe2['Ticket Price']\n",
    "    \n",
    "    dataframe2.to_excel('dataframe2.xlsx', index = False)\n",
    "    \n",
    "    return dataframe2\n",
    "\n",
    "#Create the dataframes\n",
    "dataframe1, dataframe2 = read_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the revenues of the tours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revenue_per_tour():\n",
    "    # Now group by 'product_code' and calculate the sum and avg of 'Revenue' and number of travellers\n",
    "    Revenue = dataframe2.groupby('product_code').agg({'num_of_travellers': 'sum', 'Revenue': ['sum']}).reset_index()\n",
    "    \n",
    "    # Calculate the average profit per tour\n",
    "    Revenue['Average Revenue'] = Revenue['Revenue']['sum'] / Revenue['num_of_travellers']['sum']\n",
    "    \n",
    "    # Rename the columns for clarity\n",
    "    Revenue.columns = ['product_code', 'Total Travellers', 'Total Revenue', 'Average Revenue']\n",
    "    \n",
    "    # Swap the places of the last 2 columns\n",
    "    Revenue = Revenue[['product_code', 'Total Travellers', 'Average Revenue', 'Total Revenue']]\n",
    "    \n",
    "    # Save the grouped data to a new Excel file\n",
    "    Revenue.to_excel(output_loc + 'grouped_revenue.xlsx', index=False)\n",
    "    \n",
    "\n",
    "revenue_per_tour()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the clusters and get their data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testk_means():\n",
    "    copy = dataframe2.copy()\n",
    "    \n",
    "    # Encode the \"tours\" column with label encoder\n",
    "    le_product_country = LabelEncoder()\n",
    "    le_language = LabelEncoder()\n",
    "    copy['product_country'] = le_product_country.fit_transform(copy['product_country'])\n",
    "    copy['num_of_travellers'] = le_language.fit_transform(copy['num_of_travellers'])\n",
    "    \n",
    "    # Convert month names to numerical values\n",
    "    month_mapping = {\n",
    "        'January': 1, 'February': 2, 'March': 3, 'April': 4,\n",
    "        'May': 5, 'June': 6, 'July': 7, 'August': 8,\n",
    "        'September': 9, 'October': 10, 'November': 11, 'December': 12\n",
    "    }\n",
    "    copy['Source Sheet'] = copy['Source Sheet'].map(month_mapping)\n",
    "    \n",
    "    # Select features for clustering\n",
    "    features = ['num_of_travellers', 'Revenue', 'Source Sheet', 'product_country']\n",
    "    X = copy[features]\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Determine the optimal number of clusters using the Elbow Method\n",
    "    wcss = []\n",
    "    for i in range(1, 11):\n",
    "        kmeans = KMeans(n_clusters=i, random_state=0, n_init=10)\n",
    "        kmeans.fit(X_scaled)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    \n",
    "    plt.plot(range(1, 11), wcss)\n",
    "    plt.title('Elbow Method')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate the silhouette score for k=4 (or any other number you choose)\n",
    "    k = 3\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=10).fit(X_scaled)\n",
    "    silhouette_avg = silhouette_score(X_scaled, kmeans.labels_)\n",
    "    print(f'Silhouette Score for k={k}: {silhouette_avg}')\n",
    "    \n",
    "    # Add cluster labels to the dataframe\n",
    "    copy['cluster'] = kmeans.labels_\n",
    "    \n",
    "    # Return the product country and language to their original values\n",
    "    copy['product_country'] = le_product_country.inverse_transform(copy['product_country'])\n",
    "    copy['num_of_travellers'] = le_language.inverse_transform(copy['num_of_travellers'])\n",
    "    \n",
    "    # Reverse the mapping for the month column\n",
    "    month_mapping = {v: k for k, v in month_mapping.items()}\n",
    "    copy['Source Sheet'] = copy['Source Sheet'].map(month_mapping)\n",
    "    \n",
    "    #sort the dataframe by id\n",
    "    copy = copy.sort_values(by='id')\n",
    "    \n",
    "    # Save the clustered data to an Excel file\n",
    "    copy.to_excel(output_loc + 'clustered_products.xlsx', index=False)\n",
    "    \n",
    "#add the cluster to dataframe 2 xslx\n",
    "def add_cluster():\n",
    "    \n",
    "    #kmeans check if exists\n",
    "    if not os.path.exists(output_loc + 'clustered_products.xlsx'):\n",
    "        testk_means()\n",
    "    \n",
    "    #load the clustered_products.xlsx file\n",
    "    clustered = pd.read_excel(output_loc + 'clustered_products.xlsx')\n",
    "    \n",
    "    #load the dataframe2.xlsx file\n",
    "    dataframe2 = pd.read_excel('dataframe2.xlsx')\n",
    "    \n",
    "    #add the cluster column to dataframe2\n",
    "    dataframe2['cluster'] = clustered['cluster']\n",
    "    \n",
    "    #save the results to an excel file\n",
    "    clustered.to_excel('dataframe2.xlsx', index=False)\n",
    "    \n",
    "    #drop all the column but the cluster and product_code\n",
    "    clustered = clustered[['cluster', 'product_code']]\n",
    "    \n",
    "    #return the dataframe2\n",
    "    return dataframe2\n",
    "\n",
    "def cluster_data():\n",
    "    #from dataframe2 make a copy of it\n",
    "    df2_copy = dataframe2.copy()\n",
    "    \n",
    "    #totals\n",
    "    total_travellers = df2_copy['num_of_travellers'].sum()\n",
    "    total_revenue = df2_copy['Revenue'].sum()\n",
    "    total_tours = (df2_copy['tours'].str.count(',') + 1).sum()\n",
    "    \n",
    "    #group by cluster and calculate the percentages of num_of_travellers from the total travellers\n",
    "    cluster_travellers = df2_copy.groupby('cluster')['num_of_travellers'].agg(['sum']).reset_index()\n",
    "    \n",
    "    #group by cluster and calculate the sum\n",
    "    cluster_revenue = df2_copy.groupby('cluster')['Revenue'].agg(['sum']).reset_index()\n",
    "    \n",
    "    #create a new column that counts the number of tours for each row\n",
    "    df2_copy['tours_count'] = df2_copy['tours'].str.count(',') + 1\n",
    "    \n",
    "    #group by cluster and calculate the sum of tours\n",
    "    cluster_tours = df2_copy.groupby('cluster')['tours_count'].agg(['sum']).reset_index()\n",
    "    \n",
    "    #divide all the values with the total values and multiply them by 100\n",
    "    cluster_travellers['sum'] = cluster_travellers['sum'] / total_travellers * 100\n",
    "    cluster_revenue['sum'] = cluster_revenue['sum'] / total_revenue * 100\n",
    "    cluster_tours['sum'] = cluster_tours['sum'] / total_tours * 100\n",
    "    \n",
    "    \n",
    "    #final\n",
    "    cluster_final_revenue = cluster_revenue\n",
    "    cluster_final_travellers = cluster_travellers\n",
    "    cluster_final_tours = cluster_tours\n",
    "    \n",
    "    \n",
    "    \n",
    "    #save everything on an excel file on different sheets\n",
    "    with pd.ExcelWriter(output_loc +'cluster_data.xlsx') as writer:\n",
    "        cluster_final_travellers.to_excel(writer, sheet_name='cluster_final_travellers', index=False)\n",
    "        cluster_final_revenue.to_excel(writer, sheet_name='cluster_final_revenue', index=False)\n",
    "        cluster_final_tours.to_excel(writer, sheet_name='cluster_final_tours', index=False)\n",
    "        \n",
    "dataframe2 = add_cluster()\n",
    "\n",
    "#cluster manipulation\n",
    "cluster_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the bubble chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bubble_chart_places():\n",
    "    #create a copy of the dataframe2 to operate upon\n",
    "    bubble_chart = dataframe2.copy()\n",
    "    \n",
    "    #group by cluster and product_country and calculate the sum of num_of_travellers\n",
    "    bubble_chart = bubble_chart.groupby(['cluster', 'product_country'])['num_of_travellers'].sum().reset_index()\n",
    "    \n",
    "    #sort by num_of_travellers in descending order\n",
    "    bubble_chart = bubble_chart.sort_values(by=['num_of_travellers'], ascending=False)\n",
    "    \n",
    "    #save the results to an excel file\n",
    "    bubble_chart.to_excel(output_loc + 'bubble_chart.xlsx', index=False)\n",
    "    \n",
    "    #make a bubble chart\n",
    "    fig, ax = plt.subplots()\n",
    "    for cluster in bubble_chart['cluster'].unique():\n",
    "        data = bubble_chart[bubble_chart['cluster'] == cluster]\n",
    "        ax.scatter(data['product_country'], data['cluster'], s=data['num_of_travellers'], label=f'Cluster {cluster}', alpha=0.5)\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    ax.set_xlabel('Country')\n",
    "    ax.set_ylabel('Cluster')\n",
    "    ax.set_title('Bubble Chart of Travellers by Country and Cluster')\n",
    "    # make a border around the chart\n",
    "    \n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_loc + 'bubble_chart.png')\n",
    "    plt.show()\n",
    "    \n",
    "bubble_chart_places()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization of stars for for total number of tourists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommended_stories():\n",
    "    # For each product code group by the 'Overall Experience' column and count how many times each rating appears while keeping the product code as a column\n",
    "    recommended = dataframe1.groupby(['product_code', 'Overall Experience']).size().unstack(fill_value=0).reset_index()\n",
    "    \n",
    "    #clear all the spaces from recommended the product_code column\n",
    "    recommended['product_code'] = recommended['product_code'].str.strip()\n",
    "    \n",
    "    recommended.to_excel(output_loc + 'recommended_by_stars.xlsx', index=False)\n",
    "    \n",
    "    #Load the grouped_profit.xslx file\n",
    "    grouped_profit = pd.read_excel(output_loc + 'grouped_profit.xlsx')\n",
    "    \n",
    "    #merge the grouped_profit with the recommended dataframe\n",
    "    grouped_profit = pd.merge(recommended, grouped_profit, on='product_code', how='left')\n",
    "            \n",
    "    #save the grouped_profit to an excel file\n",
    "    grouped_profit.to_excel(output_loc + 'recommended_by_stars&profit.xlsx', index=False)\n",
    "    \n",
    "    stars()\n",
    "    \n",
    "def stars():\n",
    "    #load the recommended_by_stars&profit.xlsx file\n",
    "    recommended = pd.read_excel(output_loc + 'recommended_by_stars&profit.xlsx')\n",
    "    \n",
    "    #calculate the sum of the 2nd, 3rd, 4th, 5th and 6th columns of each row\n",
    "    recommended['sum'] = recommended.iloc[:, 1:6].sum(axis=1)\n",
    "\n",
    "    #divide the sum column with the 'Total Travellers' column\n",
    "    recommended['percentage'] = recommended['sum'] / recommended['Total Travellers']\n",
    "    \n",
    "    #for each row and for the 2nd, 3rd, 4th, 5th and 6th columns multiply by the percentage column\n",
    "    recommended.iloc[:, 1:6] = recommended.iloc[:, 1:6] * recommended['percentage'].values[:, None]\n",
    "    \n",
    "    #multiply the 2nd, 3rd, 4th, 5th and 6th columns by 1 2 3 4 5 respectively\n",
    "    recommended.iloc[:, 1:6] = recommended.iloc[:, 1:6].multiply([1, 2, 3, 4, 5], axis=1)\n",
    "    \n",
    "    #calculate the sum of the 2nd, 3rd, 4th, 5th and 6th columns of each row\n",
    "    recommended['sum'] = recommended.iloc[:, 1:6].sum(axis=1)\n",
    "    \n",
    "    #export the results to an excel file\n",
    "    recommended.to_excel(output_loc + 'recommended_by_stars&profit&percentage.xlsx', index=False)\n",
    "\n",
    "recommended_stories()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of tours per packet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimum_number_of_stories():\n",
    "    #create a copy of the dataframe2 to operate upon\n",
    "    dataframe2_copy = dataframe2.copy()\n",
    "    \n",
    "    dataframe2_copy['tours'] = dataframe2_copy['tours'].str.count(',') + 1\n",
    "    \n",
    "    #to excel\n",
    "    dataframe2_copy.to_excel(output_loc + 'tours_to_numbers.xlsx', index=False)\n",
    "    \n",
    "    #turn the cluster column to int\n",
    "    dataframe2_copy['cluster'] = dataframe2_copy['cluster'].astype(int)\n",
    "    \n",
    "    #group by cluster and tours and count the instances\n",
    "    optimum_number_of_stories = dataframe2_copy.groupby(['cluster', 'tours']).size().reset_index()\n",
    "    \n",
    "    optimum_number_of_stories.to_excel(output_loc + 'optimum_number_of_stories.xlsx', index=False)\n",
    "    \n",
    "    #group by cluster and tours and calculate the sum of Revenue\n",
    "    optimum_number_of_stories_revenue = dataframe2_copy.groupby(['cluster', 'tours', 'product_country'])['Revenue'].sum().reset_index()\n",
    "    optimum_number_of_stories_revenue.to_excel(output_loc + 'optimum_number_of_stories_revenue.xlsx', index=False)\n",
    "\n",
    "optimum_number_of_stories()"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4th slide: \n",
    "\n",
    "Make a copy of recommended_by_stars&profit&percentage to a new file. With VLOOKUP find the clusters.\n",
    "\n",
    "Grade the normalized stars from 1 to 100 with 100 being the best and 1 the lowest.\n",
    "\n",
    "Grade the revenue the same way after nomralizing it first.\n",
    "\n",
    "Lastly add the 2 grades of stars and revenues and you get the grading for the clusters.\n",
    "\n",
    "Formulas:\n",
    "* Normalization:\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "* Grading:\n",
    "![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5th slide:\n",
    "\n",
    "1. Travellers: Sum of cluster's number of tourists / total tourists\n",
    "2. Revenou: Sum of cluster's total revenue / total revenue\n",
    "3. Tours: Number of cluster's tours / total tours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6th slide:\n",
    "\n",
    "Sum of cluster's total stars / cluster's total number of tourists"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
